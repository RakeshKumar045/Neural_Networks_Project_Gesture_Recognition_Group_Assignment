{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# from scipy.misc import imread, imresize\n",
    "from skimage import io\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "\n",
    "batch_size = 10#experiment with the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 120\n",
    "cols = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_Name</th>\n",
       "      <th>Action_name</th>\n",
       "      <th>Action_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WIN_20180926_16_54_08_Pro_Right_Swipe_new</td>\n",
       "      <td>Right_Swipe_new</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WIN_20180925_18_02_58_Pro_Thumbs_Down_new</td>\n",
       "      <td>Thumbs_Down_new</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WIN_20180925_17_33_08_Pro_Left_Swipe_new</td>\n",
       "      <td>Left_Swipe_new</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WIN_20180925_17_51_17_Pro_Thumbs_Up_new</td>\n",
       "      <td>Thumbs_Up_new</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WIN_20180926_17_17_35_Pro_Left_Swipe_new</td>\n",
       "      <td>Left_Swipe_new</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Video_Name      Action_name Action_Class\n",
       "0  WIN_20180926_16_54_08_Pro_Right_Swipe_new  Right_Swipe_new            1\n",
       "1  WIN_20180925_18_02_58_Pro_Thumbs_Down_new  Thumbs_Down_new            3\n",
       "2   WIN_20180925_17_33_08_Pro_Left_Swipe_new   Left_Swipe_new            0\n",
       "3    WIN_20180925_17_51_17_Pro_Thumbs_Up_new    Thumbs_Up_new            4\n",
       "4   WIN_20180926_17_17_35_Pro_Left_Swipe_new   Left_Swipe_new            0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame()\n",
    "frame_name = []\n",
    "frame_target = []\n",
    "frame_class = []\n",
    "\n",
    "\n",
    "for train_data in train_doc:\n",
    "    data = train_data.split(\";\")\n",
    "    frame_name.append(data[0])\n",
    "    frame_target.append(data[1])\n",
    "    frame_class.append(data[2].strip())\n",
    "   \n",
    "train_df[\"Video_Name\"] = frame_name\n",
    "train_df[\"Action_name\"] = frame_target\n",
    "train_df[\"Action_Class\"] = frame_class\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.Action_Class.value_counts()\n",
    "\n",
    "size = [train_df.Action_Class.value_counts()[0],train_df.Action_Class.value_counts()[1],\n",
    "        train_df.Action_Class.value_counts()[2],train_df.Action_Class.value_counts()[3],\n",
    "        train_df.Action_Class.value_counts()[4]]\n",
    "lables = ['3','1','0','2','4']\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.pie(size,labels=lables,shadow=True)\n",
    "plt.title('Pie')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Bar')\n",
    "train_df.groupby('Action_Class').Action_name.count().plot(kind='bar') #Balanced\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Crop & Resize : Many technique(open-cv, scipy, skimage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data resizing - Resizing all the images, so we can have all the images in a specific size\n",
    "def crop_resize_img(img):\n",
    "    if img.shape[0] != img.shape[1]:\n",
    "        img=img[0:120,10:150]  #crop the image\n",
    "#     resized_image = cv2.imresize(img, (rows,cols)) # resize the image\n",
    "    resized_image = cv2.resize(img, (rows,cols)) # resize the image\n",
    "    return resized_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = list(range(5,25)) #create a list of image numbers you want to use for a particular video\n",
    "    x = len(img_idx)\n",
    "    y = 120\n",
    "    z = 120\n",
    "    \n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "#         if len(t)%batch_size==0:\n",
    "        num_batches = int(len(t)//batch_size) # calculate the number of batches\n",
    "#         else:\n",
    "#             num_batches = int(len(t)//batch_size + 1)\n",
    "#             rest_videos_count = int(len(t)%batch_size)\n",
    "            \n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = io.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized_img = crop_resize_img(image) #resizing the image\n",
    "                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = resized_img[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = resized_img[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = resized_img[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            \n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if int(len(t)%batch_size!=0):\n",
    "            rest_videos_count = int(len(t)%batch_size)\n",
    "            for folder in range(rest_videos_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + ((num_batches)*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = io.imread(source_path+'/'+ t[folder + ((num_batches)*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized_img = crop_resize_img(image) #resizing the image\n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = resized_img[:,:,0]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = resized_img[:,:,1]/255 #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = resized_img[:,:,2]/255 #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "# train_path = '/notebooks/storage/Final_data/Collated_training/train'\n",
    "# val_path = '/notebooks/storage/Final_data/Collated_training/val'\n",
    "\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10# choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Bidirectional,LSTM, SimpleRNN, GRU\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 20\n",
    "rows = 120\n",
    "cols = 120\n",
    "channel_color = 3\n",
    "num_classes = 5\n",
    "input_shape = (frame, rows, cols, channel_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'> Model 1 : Vanilla Conv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(32, kernel_size = (3,3,3), padding = \"same\", activation = \"relu\" , input_shape = input_shape))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#HL1\n",
    "model.add(Conv3D(64, kernel_size=(3,3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "#HL2\n",
    "model.add(Conv3D(128, kernel_size=(3,3,3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "#Fully connected\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,  activation = \"relu\"))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(num_classes, activation = \"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 20, 120, 120, 32)  2624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 120, 120, 32)  128       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 20, 120, 120, 64)  55360     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 120, 120, 64)  256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 10, 60, 60, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 10, 60, 60, 128)   221312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 10, 60, 60, 128)   512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 5, 30, 30, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               294912512 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 295,195,269\n",
      "Trainable params: 295,194,821\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"adam\"#write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor = \"val_loss\", fractor = 0.5, patience = 2, cooldown = 1, verbose = 1)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch value:  21\n",
      "Val Epoch value:  4\n"
     ]
    }
   ],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "    \n",
    "    \n",
    "print(\"Train Epoch value: \", steps_per_epoch)\n",
    "print(\"Val Epoch value: \", validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/val ; batch size = 32\n",
      "Source path =  Project_data/train ; batch size = 32\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,20,120,120,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv3d_2/convolution = Conv3D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/batch_normalization_1/cond/Merge_grad/cond_grad\"], data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_1/cond/Merge, conv3d_2/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/categorical_accuracy/Mean/_255 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1995_metrics/categorical_accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3f8737701920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n\u001b[0m",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/user/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,20,120,120,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv3d_2/convolution = Conv3D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/batch_normalization_1/cond/Merge_grad/cond_grad\"], data_format=\"NDHWC\", dilations=[1, 1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1, 1], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_normalization_1/cond/Merge, conv3d_2/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: metrics/categorical_accuracy/Mean/_255 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1995_metrics/categorical_accuracy/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))\n",
    "    axes[0].plot(history.history['loss'])   \n",
    "    axes[0].plot(history.history['val_loss'])\n",
    "    axes[0].legend(['loss','val_loss'], loc='upper left')\n",
    "\n",
    "    axes[1].plot(history.history['categorical_accuracy'])   \n",
    "    axes[1].plot(history.history['val_categorical_accuracy'])\n",
    "    axes[1].legend(['categorical_accuracy','val_categorical_accuracy'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Method: CNN + RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + SimpleRNN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape = input_shape))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(SimpleRNN(256))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "optimiser = 'adam'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_simpleRNN = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_cnn_simpleRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_simpleRNN.history['val_loss'][-1] , history_cnn_simpleRNN.history['val_categorical_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_simpleRNN.history['loss'][-1] , history_cnn_simpleRNN.history['categorical_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_detail_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_detail_dic = {\"Model Name\" : \"CNN + SimpleRNN\",\n",
    "                   \"Model Accuracy\" : history_cnn_simpleRNN.history['categorical_accuracy'][-1],\n",
    "                   \"Model Loss\" : history_cnn_simpleRNN.history['loss'][-1] ,\n",
    "                   \"Val Accuracy\" : history_cnn_simpleRNN.history['val_categorical_accuracy'][-1],\n",
    "                   \"Val Loss\" : history_cnn_simpleRNN.history['val_loss'][1]}\n",
    "\n",
    "df = pd.DataFrame([model_detail_dic])\n",
    "model_detail_df = pd.concat([model_detail_df,df])\n",
    "print(model_detail_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_detail_df = pd.concat([model_detail_df,df])\n",
    "model_detail_df.reset_index(inplace=True)\n",
    "\n",
    "model_detail_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(accuracy, loss, val_accuracy, val_loss):\n",
    "    global model_detail_dic\n",
    "    model_detail_dic = {\"Model Name\" : \"CNN + SimpleRNN\",\n",
    "                   \"Model Accuracy\" : accuracy * 100,\n",
    "                   \"Model Loss\" : loss * 100 ,\n",
    "                   \"Val Accuracy\" : val_accuracy * 100,\n",
    "                   \"Val Loss\" : val_loss * 100}\n",
    "\n",
    "    df = pd.DataFrame([model_detail_dic])\n",
    "    df1 = pd.concat([model_detail_df,df])\n",
    "    model_detail_dic = df1.copy()\n",
    "    print(model_detail_df.head(20))\n",
    "#     return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result(history_cnn_simpleRNN.history['categorical_accuracy'][-1], history_cnn_simpleRNN.history['loss'][-1] , history_cnn_simpleRNN.history['val_categorical_accuracy'][-1], history_cnn_simpleRNN.history['val_loss'][-1])\n",
    "\n",
    "\n",
    "# model_detail_df = pd.concat([model_detail_df,df1])\n",
    "# print(model_detail_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape = input_shape))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(LSTM(128))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "optimiser = 'adam'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_lstm = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_cnn_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + GRU Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape = input_shape))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(GRU(128))\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "optimiser = 'adam'\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_gru = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_cnn_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
